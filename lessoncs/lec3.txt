scala> var lines = sc.parallelize(List("this is good good"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[15] at parallelize at <console>:23

scala> lines.flatMap(_.split("\\s+")).map((_,1)).groupByKey.collect
res7: Array[(String, Iterable[Int])] = Array((is,CompactBuffer(1)), (good,CompactBuffer(1, 1)), (this,CompactBuffer(1)))


scala> val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[19] at parallelize at <console>:23

scala> val b = a.keyBy(_.length)
b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[20] at keyBy at <console>:23

scala> b.groupByKey.collect
res8: Array[(Int, Iterable[String])] = Array((4,CompactBuffer(lion)), (6,CompactBuffer(spider)), (3,CompactBuffer(dog, cat)), (5,CompactBuffer(tiger, eagle)))

///////////////

scala> var lines = sc.parallelize(List("this is good good"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[22] at parallelize at <console>:23

scala> var r2 = lines.flatMap(_.split("\\s+")).map((_,1)).groupBy(t=>t._1)
r2: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[26] at groupBy at <console>:23

scala> var r3 = lines.flatMap(_.split("\\s+")).map((_,1)).groupBy(t=>t._1).map(t=>(t._1, t._2.size)
).collect
r3: Array[(String, Int)] = Array((is,1), (good,2), (this,1))

scala> val a = sc.parallelize(1 to 9, 3)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at parallelize at <console>:23

scala> val r3 = a.groupBy(x=> {if(x%2==0) "even" else "odd"}).collect
r3: Array[(String, Iterable[Int])] = Array((even,CompactBuffer(2, 4, 6, 8)), (odd,CompactBuffer(1, 3, 5, 7, 9)))

//////////////

scala> var lines = sc.parallelize(List("this is good good"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[39] at parallelize at <console>:23

scala> val r1 = lines.flatMap(_.split("\\s+")).map((_,1)).reduceByKey(_+_).collect
r1: Array[(String, Int)] = Array((is,1), (good,2), (this,1))

//////////////

scala> val rr = lines.flatMap(_.split("\\s+")).map((_,1)).aggregateByKey(0)(_+_,_+_).collect
rr: Array[(String, Int)] = Array((is,1), (good,2), (this,1))

scala> val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[47] at parallelize at <console>:23

scala> val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)
agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[48] at aggregateByKey at <console>:23

scala> agg.collect
res9: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))

scala> agg.partitions.size
res10: Int = 3

scala> val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[49] at parallelize at <console>:23

scala> val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect
agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3))

//////////////

scala> lines.flatMap(_.split("\\s+")).map((_,1)).aggregateByKey(0)(_+_,_+_).sortByKey(true).collect

res12: Array[(String, Int)] = Array((good,2), (is,1), (this,1))

scala> lines.flatMap(_.split("\\s+")).map((_,1)).aggregateByKey(0)(_+_,_+_).sortByKey(false).collec
t
res13: Array[(String, Int)] = Array((this,1), (is,1), (good,2))

//////////////

scala> var lines = sc.parallelize(List("this is good good"))
lines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[64] at parallelize at <console>:23

scala> lines.flatMap(_.split("\\s+")).map((_,1)).aggregateByKey(0)(_+_,_+_).sortBy(_._2,false).coll
ect
res14: Array[(String, Int)] = Array((good,2), (is,1), (this,1))

scala> lines.flatMap(_.split("\\s+")).map((_,1)).aggregateByKey(0)(_+_,_+_).sortBy(t=>t,false).coll
ect
res15: Array[(String, Int)] = Array((this,1), (is,1), (good,2))

//////////////

